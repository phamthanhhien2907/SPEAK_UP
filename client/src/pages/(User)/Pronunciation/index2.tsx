import { JSX, useState } from "react";

// C·∫•u h√¨nh API base URL
const API_BASE_URL = "http://localhost:8000";

// Kh·ªüi t·∫°o SpeechRecognition (t∆∞∆°ng th√≠ch v·ªõi tr√¨nh duy·ªát)
const SpeechRecognition =
  (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
function Pronunciation() {
  const [conversations, setConversations] = useState<JSX.Element[]>([]);
  const [loading, setLoading] = useState(false);
  const [audioLoading, setAudioLoading] = useState(false);
  const [recording, setRecording] = useState(false);
  const [recognizedText, setRecognizedText] = useState("");

  // Ki·ªÉm tra tr√¨nh duy·ªát h·ªó tr·ª£ SpeechRecognition
  const recognition = SpeechRecognition ? new SpeechRecognition() : null;

  if (recognition) {
    recognition.lang = "en-US"; // Ng√¥n ng·ªØ nh·∫≠n di·ªán (c√≥ th·ªÉ ƒë·ªïi th√†nh "vi-VN" n·∫øu c·∫ßn)
    recognition.interimResults = false; // Kh√¥ng nh·∫≠n k·∫øt qu·∫£ t·∫°m th·ªùi
    recognition.maxAlternatives = 1; // Ch·ªâ l·∫•y k·∫øt qu·∫£ t·ªët nh·∫•t
  }

  const startRecognition = () => {
    if (!recognition) {
      setConversations((prev) => [
        ...prev,
        <p key={prev.length}>Tr√¨nh duy·ªát kh√¥ng h·ªó tr·ª£ nh·∫≠n di·ªán gi·ªçng n√≥i.</p>,
      ]);
      return;
    }

    setRecording(true);
    setRecognizedText("");
    recognition.start();

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      console.log("VƒÉn b·∫£n nh·∫≠n di·ªán:", transcript);
      setRecognizedText(transcript);
      setRecording(false);
      handleSendRequest(transcript); // G·ª≠i vƒÉn b·∫£n l√™n server
    };

    recognition.onerror = (event) => {
      console.error("L·ªói nh·∫≠n di·ªán gi·ªçng n√≥i:", event.error);
      setConversations((prev) => [
        ...prev,
        <p key={prev.length}>L·ªói nh·∫≠n di·ªán gi·ªçng n√≥i: {event.error}</p>,
      ]);
      setRecording(false);
    };

    recognition.onend = () => {
      setRecording(false);
    };
  };

  const stopRecognition = () => {
    if (recognition && recording) {
      recognition.stop();
      setRecording(false);
    }
  };

  const handleSendRequest = async (text: string) => {
    setLoading(true);
    try {
      console.log("G·ª≠i vƒÉn b·∫£n l√™n server:", text);
      const res = await fetch(`${API_BASE_URL}/text-and-respond/`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ user_text: text }),
      });

      if (!res.ok) throw new Error(`Y√™u c·∫ßu th·∫•t b·∫°i v·ªõi m√£ ${res.status}`);

      const data = await res.json();
      console.log("Ph·∫£n h·ªìi t·ª´ API:", data);
      const newConversation = (
        <p key={conversations.length}>
          üßë B·∫°n: {data.user_text}
          <br />
          ü§ñ AI: {data.ai_response}
        </p>
      );
      setConversations((prev) => [...prev, newConversation]);

      // Ph√°t √¢m thanh n·∫øu c√≥ audio_url
      if (data.audio_url) {
        setAudioLoading(true);
        const audio = new Audio(`${API_BASE_URL}${data.audio_url}`);
        audio.onloadeddata = () => {
          console.log("√Çm thanh ƒë√£ t·∫£i th√†nh c√¥ng.");
          setAudioLoading(false);
        };
        audio.onended = () => {
          setConversations((prev) => [
            ...prev,
            <p key={prev.length}>√Çm thanh ƒë√£ ph√°t th√†nh c√¥ng.</p>,
          ]);
        };
        audio.onerror = (event: any) => {
          console.error("L·ªói ph√°t √¢m thanh:", event);
          setConversations((prev) => [
            ...prev,
            <p key={prev.length}>
              L·ªói ph√°t √¢m thanh: {event.message || "Kh√¥ng x√°c ƒë·ªãnh"}
            </p>,
          ]);
          setAudioLoading(false);
        };
        audio.play().catch((err) => {
          console.error("L·ªói khi ph√°t √¢m thanh:", err);
          setConversations((prev) => [
            ...prev,
            <p key={prev.length}>L·ªói ph√°t √¢m thanh: {err.message}</p>,
          ]);
          setAudioLoading(false);
        });
      }
    } catch (err: any) {
      console.error("L·ªói g·ª≠i y√™u c·∫ßu:", err);
      setConversations((prev) => [
        ...prev,
        <p key={prev.length}>C√≥ l·ªói khi g·ª≠i y√™u c·∫ßu API: {err.message}</p>,
      ]);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="min-h-screen bg-gray-100 p-4">
      <h2 className="text-2xl font-bold mb-4 text-center">
        G·ªçi tr·ª£ l√Ω h·ªçc ti·∫øng Anh
      </h2>
      <div className="flex space-x-4 mb-4">
        <button
          onClick={startRecognition}
          disabled={recording || loading}
          className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
        >
          {recording ? "ƒêang nh·∫≠n di·ªán..." : "B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán"}
        </button>
        <button
          onClick={stopRecognition}
          disabled={!recording || loading}
          className="px-4 py-2 bg-red-500 text-white rounded disabled:bg-red-300"
        >
          D·ª´ng nh·∫≠n di·ªán
        </button>
      </div>
      <div className="mb-4">
        <p className="text-gray-700">
          VƒÉn b·∫£n nh·∫≠n di·ªán: {recognizedText || "Ch∆∞a c√≥ d·ªØ li·ªáu"}
        </p>
      </div>
      <div
        className="h-full bg-white p-4 rounded shadow-md overflow-y-auto"
        style={{ maxHeight: "60vh" }}
      >
        {conversations.map((conv, index) => (
          <div key={index} className="mb-2">
            {conv}
          </div>
        ))}
      </div>
      {loading && (
        <p className="mt-2 text-center text-gray-600">ƒêang x·ª≠ l√Ω...</p>
      )}
      {audioLoading && (
        <p className="mt-2 text-center text-gray-600">ƒêang t·∫£i √¢m thanh...</p>
      )}
    </div>
  );
}

export default Pronunciation;
