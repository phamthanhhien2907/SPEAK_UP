import { useState, useRef } from "react";

function Pronunciation() {
  const [conversations, setConversations] = useState<string[]>([]);
  const [loading, setLoading] = useState(false);
  const [recording, setRecording] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      mediaRecorderRef.current = new MediaRecorder(stream);
      audioChunksRef.current = [];

      mediaRecorderRef.current.ondataavailable = (event) => {
        console.log("D·ªØ li·ªáu √¢m thanh nh·∫≠n ƒë∆∞·ª£c:", event.data.size);
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorderRef.current.onstop = async () => {
        console.log("Ghi √¢m ƒë√£ d·ª´ng, x·ª≠ l√Ω d·ªØ li·ªáu...");
        if (audioChunksRef.current.length === 0) {
          setConversations((prev) => [
            ...prev,
            "Kh√¥ng c√≥ d·ªØ li·ªáu √¢m thanh ƒë·ªÉ g·ª≠i.",
          ]);
          return;
        }

        const audioBlob = new Blob(audioChunksRef.current, {
          type: "audio/wav",
        });
        const reader = new FileReader();
        reader.readAsDataURL(audioBlob);
        reader.onloadend = async () => {
          const base64Audio = reader.result?.toString().split(",")[1];
          if (!base64Audio) {
            setConversations((prev) => [
              ...prev,
              "Kh√¥ng th·ªÉ chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu √¢m thanh.",
            ]);
            return;
          }
          await handleSendRequest(base64Audio);
        };

        streamRef.current?.getTracks().forEach((track) => track.stop());
        streamRef.current = null;
      };

      mediaRecorderRef.current.onerror = (event) => {
        console.error("L·ªói MediaRecorder:", event);
        setConversations((prev) => [...prev, "L·ªói ghi √¢m, vui l√≤ng th·ª≠ l·∫°i."]);
        setRecording(false);
      };

      mediaRecorderRef.current.start();
      console.log("B·∫Øt ƒë·∫ßu ghi √¢m...");
      setRecording(true);
    } catch (err) {
      console.error("L·ªói b·∫Øt ƒë·∫ßu ghi √¢m:", err);
      setConversations((prev) => [
        ...prev,
        "Kh√¥ng th·ªÉ truy c·∫≠p micro, ki·ªÉm tra quy·ªÅn.",
      ]);
      setRecording(false);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && recording) {
      console.log("D·ª´ng ghi √¢m...");
      mediaRecorderRef.current.stop();
      setRecording(false);
    } else {
      console.log("Kh√¥ng c√≥ ghi √¢m ƒë·ªÉ d·ª´ng.");
    }
  };

  const handleSendRequest = async (base64Audio: string) => {
    setLoading(true);
    try {
      console.log("G·ª≠i y√™u c·∫ßu API...");
      const res = await fetch("http://localhost:8000/transcribe-and-respond/", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({ audio_data: base64Audio }),
      });

      if (!res.ok) throw new Error(`Y√™u c·∫ßu th·∫•t b·∫°i v·ªõi m√£ ${res.status}`);

      const data = await res.json();
      console.log("Ph·∫£n h·ªìi t·ª´ API:", data);
      const newConversation = `üßë B·∫°n: ${data.user_text}\nü§ñ AI: ${data.ai_response}`;
      setConversations((prev) => [...prev, newConversation]);

      // Ph√°t √¢m thanh v√† ki·ªÉm tra
      const audio = new Audio(`http://localhost:8000${data.audio_url}`);
      audio.onended = () => {
        console.log("√Çm thanh ƒë√£ ph√°t xong.");
        setConversations((prev) => [...prev, "√Çm thanh ƒë√£ ph√°t th√†nh c√¥ng."]);
      };
      audio.onerror = (err) => {
        console.error("L·ªói ph√°t √¢m thanh:", err);
        setConversations((prev) => [
          ...prev,
          "L·ªói ph√°t √¢m thanh, ki·ªÉm tra l·∫°i.",
        ]);
      };
      audio.play().catch((err) => {
        console.error("L·ªói khi ph√°t √¢m thanh:", err);
        setConversations((prev) => [
          ...prev,
          "L·ªói ph√°t √¢m thanh, ki·ªÉm tra l·∫°i.",
        ]);
      });
    } catch (err) {
      console.error("L·ªói g·ª≠i y√™u c·∫ßu:", err);
      setConversations((prev) => [...prev, "C√≥ l·ªói khi g·ª≠i y√™u c·∫ßu API."]);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="min-h-screen bg-gray-100 p-4">
      <h2 className="text-2xl font-bold mb-4 text-center">
        G·ªçi tr·ª£ l√Ω h·ªçc ti·∫øng Anh
      </h2>
      <div className="flex space-x-4 mb-4">
        <button
          onClick={startRecording}
          disabled={recording || loading}
          className="px-4 py-2 bg-blue-500 text-white rounded disabled:bg-blue-300"
        >
          {recording ? "ƒêang ghi √¢m..." : "B·∫Øt ƒë·∫ßu ghi √¢m"}
        </button>
        <button
          onClick={stopRecording}
          disabled={!recording || loading}
          className="px-4 py-2 bg-red-500 text-white rounded disabled:bg-red-300"
        >
          D·ª´ng ghi √¢m
        </button>
      </div>
      <div
        className="h-full bg-white p-4 rounded shadow-md overflow-y-auto"
        style={{ maxHeight: "60vh" }}
      >
        {conversations.map((conv, index) => (
          <p key={index} className="mb-2">
            {conv}
          </p>
        ))}
      </div>
      {loading && (
        <p className="mt-2 text-center text-gray-600">ƒêang x·ª≠ l√Ω...</p>
      )}
    </div>
  );
}

export default Pronunciation;
